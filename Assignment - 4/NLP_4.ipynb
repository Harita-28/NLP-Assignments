{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q4) Create a transformer from scratch using the Pytorch library"
      ],
      "metadata": {
        "id": "3R4WbmMtxNK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khWR9knNr0p_",
        "outputId": "a36da6c6-14d4-40d4-f57c-3470d8142751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Position-wise Feed Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "        attention = torch.softmax(score, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "        return torch.matmul(attention, value)\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.attn = ScaledDotProductAttention(dropout)\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        query = self.query(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = self.value(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        attention = self.attn(query, key, value, mask)\n",
        "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        output = self.out(attention)\n",
        "        return output\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attn(x, x, x, mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.layernorm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_encoder_layers, d_ff, vocab_size, max_len=5000, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(src.size(-1))\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        # Encoder pass\n",
        "        memory = src\n",
        "        for layer in self.encoder_layers:\n",
        "            memory = layer(memory, src_mask)\n",
        "\n",
        "        output = self.fc_out(memory)\n",
        "        return output\n",
        "\n",
        "# Model Initialization\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_encoder_layers = 3\n",
        "d_ff = 512\n",
        "vocab_size = 100\n",
        "seq_length = 20\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model = Transformer(d_model, num_heads, num_encoder_layers, d_ff, vocab_size)\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Generate Random Training Data\n",
        "def generate_data(batch_size, seq_length, vocab_size):\n",
        "    return torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    src = generate_data(batch_size, seq_length, vocab_size)\n",
        "    tgt = generate_data(batch_size, seq_length, vocab_size)\n",
        "\n",
        "    output = model(src)\n",
        "\n",
        "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1))  # Reshape for loss function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Inference\n",
        "model.eval()\n",
        "test_input = generate_data(1, seq_length, vocab_size)  # Single test sequence\n",
        "output = model(test_input)\n",
        "predicted = torch.argmax(output, dim=-1)\n",
        "print(\"\\nPredicted Sequence:\\n\", predicted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGc3yTKyrIQQ",
        "outputId": "31dc99cb-3c2c-4eb2-f572-bd43601de276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 4.7972\n",
            "Epoch [2/5], Loss: 4.8101\n",
            "Epoch [3/5], Loss: 4.7753\n",
            "Epoch [4/5], Loss: 4.7176\n",
            "Epoch [5/5], Loss: 4.7441\n",
            "\n",
            "Predicted Sequence:\n",
            " tensor([[68,  3, 56, 71, 67, 68, 67, 71, 84, 47, 68, 68, 81, 76, 68, 27, 76, 84,\n",
            "         55,  3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM5c95z-sagb",
        "outputId": "93256adb-f9a5-40bf-d79b-851eb1ca6a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (embedding): Embedding(100, 128)\n",
            "  (positional_encoding): PositionalEncoding()\n",
            "  (encoder_layers): ModuleList(\n",
            "    (0-2): 3 x EncoderLayer(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (attn): ScaledDotProductAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (relu): ReLU()\n",
            "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=128, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}